{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFydjcxm_WMB",
        "outputId": "dbbca350-db6a-4e87-d74e-243806ede534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeVj5QLMv81x"
      },
      "source": [
        "# Bronze -> Silver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot9occSiAOYm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfkgbTsi_jYz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime\n",
        "from pyspark.sql.types import TimestampType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0wh5G6IDEFS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "class DataCleaning:\n",
        "    def __init__(self):\n",
        "        self.spark = self.get_spark_session()\n",
        "        self.jdbc_url, self.db_properties = self.get_db()\n",
        "\n",
        "    def get_spark_session(self):\n",
        "        return (\n",
        "            SparkSession.builder\n",
        "            .appName(\"colab_test\")\n",
        "            .master(\"local[*]\")\n",
        "            .config(\"spark.driver.memory\", \"8g\")\n",
        "            .config(\"spark.driver.maxResultSize\", \"2g\")\n",
        "            .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "            .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\n",
        "            .getOrCreate()\n",
        "        )\n",
        "\n",
        "    def get_db(self):\n",
        "        db_connector = {\n",
        "            \"host\": os.getenv(\"POSTGRES_HOST\"),\n",
        "            \"port\": os.getenv(\"POSTGRES_PORT\"),\n",
        "            \"dbname\": os.getenv(\"POSTGRES_DB\"),\n",
        "            \"user\": os.getenv(\"POSTGRES_USER\"),\n",
        "            \"password\": os.getenv(\"POSTGRES_PASSWORD\")\n",
        "        }\n",
        "\n",
        "        db_properties = {\n",
        "            \"user\": db_connector[\"user\"],\n",
        "            \"password\": db_connector[\"password\"],\n",
        "            \"driver\": \"org.postgresql.Driver\"\n",
        "        }\n",
        "        jdbc_url = f\"jdbc:postgresql://{db_connector['host']}:{db_connector['port']}/{db_connector['dbname']}\"\n",
        "        return jdbc_url, db_properties\n",
        "\n",
        "    # ===================== EXTRACT FUNCTIONS =====================\n",
        "\n",
        "    def table(self, table_name, column, start_day=None, end_day=None, lowerBound=1, upperBound=100000, numPartitions=8):\n",
        "        query = table_name\n",
        "        if start_day and end_day:\n",
        "            try:\n",
        "                temp_df = self.spark.read.jdbc(url=self.jdbc_url, table=table_name, properties=self.db_properties)\n",
        "                if 'created_at' in temp_df.columns:\n",
        "                    query = f\"(SELECT * FROM {table_name} WHERE created_at BETWEEN '{start_day}' AND '{end_day}') AS t\"\n",
        "                else:\n",
        "                    print(f\"Warning: 'created_at' column not found in table '{table_name}'. Skipping date filter.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error checking table schema for '{table_name}': {e}\")\n",
        "                print(\"Skipping date filter.\")\n",
        "\n",
        "\n",
        "        return self.spark.read.jdbc(\n",
        "            url=self.jdbc_url,\n",
        "            table=query,\n",
        "            column=column,\n",
        "            lowerBound=lowerBound,\n",
        "            upperBound=upperBound,\n",
        "            numPartitions=numPartitions,\n",
        "            properties=self.db_properties\n",
        "        )\n",
        "\n",
        "    # ===================== TRANSFORM FUNCTIONS =====================\n",
        "\n",
        "    def check_missing(self, df):\n",
        "        df_sample = df.sample(withReplacement=False, fraction=0.01, seed=42).cache()\n",
        "        try:\n",
        "            total_count = df_sample.count()\n",
        "            agg_exprs = [\n",
        "                F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_sample.columns\n",
        "            ]\n",
        "            missing_df = df_sample.agg(*agg_exprs)\n",
        "\n",
        "            results = []\n",
        "            row = missing_df.first().asDict()\n",
        "            for col, miss in row.items():\n",
        "                results.append({\n",
        "                    \"column\": col,\n",
        "                    \"missing_count\": miss,\n",
        "                    \"missing_ratio\": miss / total_count if total_count > 0 else 0\n",
        "                })\n",
        "            return results\n",
        "        finally:\n",
        "            df_sample.unpersist()\n",
        "\n",
        "    def check_duplicate(self, df, subset_cols, drop=False):\n",
        "        df.cache()\n",
        "        try:\n",
        "            total_count = df.count()\n",
        "            distinct_count = df.dropDuplicates(subset=subset_cols).count()\n",
        "            dup_count = total_count - distinct_count\n",
        "            dup_ratio = dup_count / total_count if total_count > 0 else 0\n",
        "\n",
        "            stats = {\n",
        "                \"duplicate_count\": dup_count,\n",
        "                \"duplicate_ratio\": dup_ratio\n",
        "            }\n",
        "\n",
        "            if drop and dup_count > 0:\n",
        "                cleaned_df = df.dropDuplicates(subset=subset_cols)\n",
        "                print(f\"Found {dup_count} duplicate rows. Dropped duplicates. New total: {distinct_count}\")\n",
        "                return stats, cleaned_df\n",
        "            else:\n",
        "                print(f\"Duplicate rows: {dup_count} ({dup_ratio:.4%})\")\n",
        "                return stats, df\n",
        "        finally:\n",
        "            df.unpersist()\n",
        "\n",
        "\n",
        "    def check_invalid_email(self, df, email_col=\"email\", drop=False):\n",
        "        df.cache()\n",
        "        try:\n",
        "            regex_pattern = r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\"\n",
        "\n",
        "            counts = df.agg(\n",
        "                F.sum(F.when(F.col(email_col).rlike(regex_pattern), 1).otherwise(0)).alias(\"valid\"),\n",
        "                F.sum(F.when(~F.col(email_col).rlike(regex_pattern), 1).otherwise(0)).alias(\"invalid\"),\n",
        "                F.count(\"*\").alias(\"total\")\n",
        "            ).collect()[0]\n",
        "\n",
        "            valid = counts[\"valid\"]\n",
        "            invalid = counts[\"invalid\"]\n",
        "            total = counts[\"total\"]\n",
        "            invalid_ratio = invalid / total if total > 0 else 0\n",
        "\n",
        "            stats = {\n",
        "                \"valid_email_count\": valid,\n",
        "                \"invalid_email_count\": invalid,\n",
        "                \"invalid_email_ratio\": invalid_ratio\n",
        "            }\n",
        "\n",
        "            if drop and invalid > 0:\n",
        "                cleaned_df = df.filter(F.col(email_col).rlike(regex_pattern))\n",
        "                print(f\"Found {invalid} invalid emails ({invalid_ratio:.4%}). Dropped invalid emails. New total: {valid}\")\n",
        "                return stats, cleaned_df\n",
        "            else:\n",
        "                print(f\"Valid emails: {valid}, Invalid emails: {invalid} ({invalid_ratio:.4%})\")\n",
        "                return stats, df\n",
        "        finally:\n",
        "            df.unpersist()\n",
        "\n",
        "    # ===================== LOAD FUNCTIONS =====================\n",
        "\n",
        "    def save_to_parquet(self, df, path: str, mode: str = \"overwrite\"):\n",
        "        df.write.mode(mode).parquet(path)\n",
        "        print(f\"Data saved to Parquet at: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogwPTMq2DaPp"
      },
      "outputs": [],
      "source": [
        "dc = DataCleaning()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgnAmOMFN1KK"
      },
      "source": [
        "# USERS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6yyusL4LDYV",
        "outputId": "6be947ff-c165-4387-daac-88ad2a24e10a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: 'created_at' column not found in table 'users'. Skipping date filter.\n"
          ]
        }
      ],
      "source": [
        "df_user = dc.table(table_name=\"users\",\n",
        "                   column=\"user_id\",\n",
        "                   start_day=\"2023-01-01\",\n",
        "                   end_day=\"2025-01-31\",\n",
        "                   lowerBound=1,\n",
        "                   upperBound=100000,\n",
        "                   numPartitions=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZjJl5SXMVZJ"
      },
      "outputs": [],
      "source": [
        "missing_report = dc.check_missing(df_user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03i51QSINeL0",
        "outputId": "ebd22959-e2ff-4156-8a18-77e30bc7d4a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'column': 'user_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'username', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'email', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'password_hash', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'full_name', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'registration_date', 'missing_count': 0, 'missing_ratio': 0.0}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7404vaSuNz8B",
        "outputId": "4237cca9-84eb-41e2-b9fe-214ec7b45138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate rows: 0 (0.0000%)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'duplicate_count': 0, 'duplicate_ratio': 0.0}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats, df_user = dc.check_duplicate(df_user, subset_cols=[\"user_id\"], drop=True)\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn9qIoThPX15",
        "outputId": "0458f552-9f5b-47b3-ed9c-f0cb87e29e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 17513 invalid emails (17.5130%). Dropped invalid emails. New total: 82487\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'valid_email_count': 82487,\n",
              " 'invalid_email_count': 17513,\n",
              " 'invalid_email_ratio': 0.17513}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats, df_user = dc.check_invalid_email(df_user, email_col=\"email\", drop=True)\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tswmO6dAQTnc",
        "outputId": "a570fb3b-209e-4510-d3f9-17d637b0a686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to Parquet at: users.parquet\n"
          ]
        }
      ],
      "source": [
        "dc.save_to_parquet(df_user, \"users.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raU16ItKQqjk"
      },
      "source": [
        "# COMMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C3d3UdhQt9h"
      },
      "outputs": [],
      "source": [
        "df_comment = dc.table(table_name=\"comments\",\n",
        "                   column=\"comment_id\",\n",
        "                   start_day=\"2023-01-01\",\n",
        "                   end_day=\"2025-01-31\",\n",
        "                   lowerBound=1,\n",
        "                   upperBound=1048700,\n",
        "                   numPartitions=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlW19QPJRA_z"
      },
      "outputs": [],
      "source": [
        "missing_report = dc.check_missing(df_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSP37mDFRFJk",
        "outputId": "8da9f33a-8730-418b-b801-13547905f79c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'column': 'comment_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'post_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'user_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'content', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'created_at', 'missing_count': 0, 'missing_ratio': 0.0}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUT77JMQRJdP",
        "outputId": "e62d8da1-aa2f-4d2a-a318-fe278a015146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 137989 duplicate rows. Dropped duplicates. New total: 701275\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'duplicate_count': 137989, 'duplicate_ratio': 0.16441667937621535}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats, df_comment = dc.check_duplicate(df_comment, subset_cols=['content'], drop=True)\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vN--nU8RZL8",
        "outputId": "45c6cfbd-4d12-4eb6-beda-3366cbc59b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to Parquet at: comments.parquet\n"
          ]
        }
      ],
      "source": [
        "dc.save_to_parquet(df_comment, \"comments.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sviJuwlRhro"
      },
      "source": [
        "# POSTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUGwuyCORkSq"
      },
      "outputs": [],
      "source": [
        "df_post = dc.table(table_name=\"posts\",\n",
        "                   column=\"post_id\",\n",
        "                   start_day=\"2023-01-01\",\n",
        "                   end_day=\"2025-01-31\",\n",
        "                   lowerBound=1,\n",
        "                   upperBound=1048750,\n",
        "                   numPartitions=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkS2dnwVRsp5",
        "outputId": "a5cb46b7-775a-4a4f-c568-adcca60aa9de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'column': 'post_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'user_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'content', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'created_at', 'missing_count': 0, 'missing_ratio': 0.0}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_report = dc.check_missing(df_post)\n",
        "missing_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RAm4i__RwcE",
        "outputId": "910fa117-5301-464d-dc1b-6470d88480c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 329663 duplicate rows. Dropped duplicates. New total: 1629714\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'duplicate_count': 329663, 'duplicate_ratio': 0.16824888727386306}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats, df_post = dc.check_duplicate(df_post, subset_cols=['content'], drop=True)\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2V_Rz9dRx-u",
        "outputId": "08cc1f5e-6f7f-43c2-8f1c-4f0d654a15bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to Parquet at: posts.parquet\n"
          ]
        }
      ],
      "source": [
        "dc.save_to_parquet(df_post, \"posts.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3gniDd6SLNt"
      },
      "source": [
        "# LIKES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq1ksJHoSNsA"
      },
      "outputs": [],
      "source": [
        "df_like = dc.table(table_name=\"likes\",\n",
        "                   column=\"like_id\",\n",
        "                   start_day=\"2023-01-01\",\n",
        "                   end_day=\"2025-01-31\",\n",
        "                   lowerBound=1,\n",
        "                   upperBound=1048750,\n",
        "                   numPartitions=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXpk_LkSSTrk",
        "outputId": "7166c4e6-2fbd-4079-f56a-2b08e60b2fdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'column': 'like_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'post_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'user_id', 'missing_count': 0, 'missing_ratio': 0.0},\n",
              " {'column': 'created_at', 'missing_count': 0, 'missing_ratio': 0.0}]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_report = dc.check_missing(df_like)\n",
        "missing_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpod2AqHSVMr",
        "outputId": "bb4eb7d3-2a21-484c-d632-3b136032f2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate rows: 0 (0.0000%)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'duplicate_count': 0, 'duplicate_ratio': 0.0}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats, df_like = dc.check_duplicate(df_like, subset_cols=['like_id', 'post_id'], drop=True)\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTuS5zaxSXME",
        "outputId": "bf70a41d-0419-4a82-8105-2968554e1cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to Parquet at: likes.parquet\n"
          ]
        }
      ],
      "source": [
        "dc.save_to_parquet(df_like, \"likes.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHo9n9WwwAiw"
      },
      "source": [
        "# Silver -> Gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPKNVw9Kixt9",
        "outputId": "abeba6b6-1c0f-4af5-b570-983efcfb8b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Added fake sentiment and keywords columns to silver datasets.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Add fake sentiment and keywords columns to silver parquet files\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "import random\n",
        "\n",
        "# ============ INIT SPARK ============\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"add_fake_sentiment_keywords\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# ============ LOAD SILVER PARQUETS ============\n",
        "posts = spark.read.parquet(\"posts.parquet\")\n",
        "comments = spark.read.parquet(\"comments.parquet\")\n",
        "\n",
        "# ============ ADD FAKE SENTIMENT ============\n",
        "# random sentiment (replace later with actual model inference)\n",
        "sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
        "\n",
        "posts = posts.withColumn(\n",
        "    \"sentiment\",\n",
        "    F.when(F.rand() < 0.33, F.lit(\"positive\"))\n",
        "     .when(F.rand() < 0.66, F.lit(\"neutral\"))\n",
        "     .otherwise(F.lit(\"negative\"))\n",
        ")\n",
        "\n",
        "comments = comments.withColumn(\n",
        "    \"sentiment\",\n",
        "    F.when(F.rand() < 0.33, F.lit(\"positive\"))\n",
        "     .when(F.rand() < 0.66, F.lit(\"neutral\"))\n",
        "     .otherwise(F.lit(\"negative\"))\n",
        ")\n",
        "\n",
        "# ============ ADD FAKE KEYWORDS ============\n",
        "keywords_list = [\n",
        "    [\"ai\", \"machine learning\", \"innovation\"],\n",
        "    [\"sports\", \"football\", \"health\"],\n",
        "    [\"travel\", \"food\", \"culture\"],\n",
        "    [\"technology\", \"mobile\", \"data\"]\n",
        "]\n",
        "\n",
        "# pick random keyword group for each record\n",
        "def random_keywords():\n",
        "    return random.choice(keywords_list)\n",
        "\n",
        "# Register UDF\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "rand_keywords_udf = udf(lambda: random.choice(keywords_list), ArrayType(StringType()))\n",
        "\n",
        "posts = posts.withColumn(\"keywords\", rand_keywords_udf())\n",
        "comments = comments.withColumn(\"keywords\", rand_keywords_udf())\n",
        "\n",
        "# ============ SAVE BACK TO UPDATED PARQUETS ============\n",
        "posts.write.mode(\"overwrite\").parquet(\"silver_posts.parquet\")\n",
        "comments.write.mode(\"overwrite\").parquet(\"silver_comments.parquet\")\n",
        "\n",
        "print(\"âœ… Added fake sentiment and keywords columns to silver datasets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGTcB3h0dyN5",
        "outputId": "13903367-931c-4773-8b5c-867714cf157c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[like_id: int, post_id: int, user_id: int, created_at: timestamp]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Final Silver â†’ Gold Transformation Script\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession, functions as F, Window\n",
        "from datetime import date\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# ===================== INIT SPARK =====================\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"silver_to_gold_final\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"8g\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# ===================== LOAD SILVER PARQUETS =====================\n",
        "posts = spark.read.parquet(\"silver_posts.parquet\")\n",
        "comments = spark.read.parquet(\"silver_comments.parquet\")\n",
        "likes = spark.read.parquet(\"likes.parquet\")\n",
        "users = spark.read.parquet(\"users.parquet\")\n",
        "\n",
        "# Ensure required columns exist\n",
        "if \"sentiment\" not in posts.columns:\n",
        "    posts = posts.withColumn(\"sentiment\", F.lit(\"neutral\"))\n",
        "if \"sentiment\" not in comments.columns:\n",
        "    comments = comments.withColumn(\"sentiment\", F.lit(\"neutral\"))\n",
        "\n",
        "posts.cache()\n",
        "comments.cache()\n",
        "likes.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1gJFcRHQa1T",
        "outputId": "e7dd95a4-b6cf-4b25-ad08-ea8f9149f716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… gold_daily_platform_summary created\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1ï¸âƒ£ GOLD: DAILY PLATFORM SUMMARY\n",
        "# ============================================================\n",
        "posts_daily = posts.groupBy(F.to_date(\"created_at\").alias(\"report_date\")).agg(\n",
        "    F.countDistinct(\"post_id\").alias(\"total_posts\"),\n",
        "    F.countDistinct(\"user_id\").alias(\"active_posters\"),\n",
        "    F.sum(F.when(F.col(\"sentiment\") == \"positive\", 1).otherwise(0)).alias(\"positive_posts_count\"),\n",
        "    F.sum(F.when(F.col(\"sentiment\") == \"negative\", 1).otherwise(0)).alias(\"negative_posts_count\")\n",
        ")\n",
        "\n",
        "comments_daily = comments.groupBy(F.to_date(\"created_at\").alias(\"report_date\")).agg(\n",
        "    F.countDistinct(\"comment_id\").alias(\"total_comments\"),\n",
        "    F.countDistinct(\"user_id\").alias(\"active_commenters\"),\n",
        "    F.sum(F.when(F.col(\"sentiment\") == \"positive\", 1).otherwise(0)).alias(\"positive_comments_count\"),\n",
        "    F.sum(F.when(F.col(\"sentiment\") == \"negative\", 1).otherwise(0)).alias(\"negative_comments_count\")\n",
        ")\n",
        "\n",
        "likes_daily = likes.groupBy(F.to_date(\"created_at\").alias(\"report_date\")).agg(\n",
        "    F.countDistinct(\"like_id\").alias(\"total_likes\"),\n",
        "    F.countDistinct(\"user_id\").alias(\"active_likers\")\n",
        ")\n",
        "\n",
        "daily_summary = (\n",
        "    posts_daily.join(comments_daily, \"report_date\", \"outer\")\n",
        "    .join(likes_daily, \"report_date\", \"outer\")\n",
        "    .fillna(0)\n",
        "    .withColumn(\"avg_comments_per_post\",\n",
        "                F.when(F.col(\"total_posts\") > 0, F.col(\"total_comments\") / F.col(\"total_posts\")).otherwise(0))\n",
        "    .withColumn(\"avg_likes_per_post\",\n",
        "                F.when(F.col(\"total_posts\") > 0, F.col(\"total_likes\") / F.col(\"total_posts\")).otherwise(0))\n",
        ")\n",
        "\n",
        "# total_active_users: distinct union of user_ids per day\n",
        "user_union = (\n",
        "    posts.select(F.to_date(\"created_at\").alias(\"report_date\"), F.col(\"user_id\"))\n",
        "    .union(comments.select(F.to_date(\"created_at\").alias(\"report_date\"), F.col(\"user_id\")))\n",
        "    .union(likes.select(F.to_date(\"created_at\").alias(\"report_date\"), F.col(\"user_id\")))\n",
        "    .distinct()\n",
        ")\n",
        "active_user_counts = user_union.groupBy(\"report_date\").agg(F.countDistinct(\"user_id\").alias(\"total_active_users\"))\n",
        "\n",
        "daily_summary = daily_summary.join(active_user_counts, \"report_date\", \"left\").fillna(0)\n",
        "\n",
        "daily_summary.repartition(\"report_date\").write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"report_date\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(\"gold_daily_platform_summary.parquet\")\n",
        "\n",
        "print(\"âœ… gold_daily_platform_summary created\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs4ADyNnQcSZ",
        "outputId": "aba64139-7b48-4d27-d07a-7a6514778cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… gold_user_snapshot created\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2ï¸âƒ£ GOLD: USER SNAPSHOT\n",
        "# ============================================================\n",
        "user_activity = (\n",
        "    users.alias(\"u\")\n",
        "    .join(\n",
        "        posts.groupBy(\"user_id\").agg(\n",
        "            F.count(\"*\").alias(\"total_posts_per_user\"),\n",
        "            F.mean(F.when(F.col(\"sentiment\") == \"positive\", 1).otherwise(0)).alias(\"positive_post_ratio\"),\n",
        "            F.sum(F.when(F.col(\"created_at\") >= F.date_sub(F.current_date(), 30), 1).otherwise(0)).alias(\"posts_in_last_30_days\")\n",
        "        ), \"user_id\", \"left\"\n",
        "    )\n",
        "    .join(\n",
        "        comments.groupBy(\"user_id\").agg(\n",
        "            F.count(\"*\").alias(\"total_comments_given\"),\n",
        "            F.sum(F.when(F.col(\"created_at\") >= F.date_sub(F.current_date(), 30), 1).otherwise(0)).alias(\"comments_in_last_30_days\")\n",
        "        ), \"user_id\", \"left\"\n",
        "    )\n",
        "    .join(\n",
        "        likes.groupBy(\"user_id\").agg(\n",
        "            F.count(\"*\").alias(\"total_likes_given\")\n",
        "        ), \"user_id\", \"left\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Received feedback\n",
        "comments_received = comments.groupBy(\"post_id\").agg(\n",
        "    F.count(\"*\").alias(\"total_comments_received\"),\n",
        "    F.mean(F.when(F.col(\"sentiment\") == \"positive\", 1).otherwise(0)).alias(\"positive_comment_received_ratio\")\n",
        ")\n",
        "likes_received = likes.groupBy(\"post_id\").agg(F.count(\"*\").alias(\"total_likes_received\"))\n",
        "\n",
        "posts_feedback = (\n",
        "    posts.join(broadcast(comments_received), \"post_id\", \"left\")\n",
        "         .join(broadcast(likes_received), \"post_id\", \"left\")\n",
        "         .groupBy(\"user_id\")\n",
        "         .agg(\n",
        "             F.sum(\"total_comments_received\").alias(\"total_comments_received\"),\n",
        "             F.sum(\"total_likes_received\").alias(\"total_likes_received\"),\n",
        "             F.avg(\"total_likes_received\").alias(\"avg_likes_per_post\"),\n",
        "             F.avg(\"total_comments_received\").alias(\"avg_comments_per_post\"),\n",
        "             F.mean(\"positive_comment_received_ratio\").alias(\"positive_comment_received_ratio\")\n",
        "         )\n",
        ")\n",
        "\n",
        "# ðŸŸ© Compute user's most recent activity date\n",
        "last_activity = (\n",
        "    posts.select(\"user_id\", \"created_at\")\n",
        "    .union(comments.select(\"user_id\", \"created_at\"))\n",
        "    .union(likes.select(\"user_id\", \"created_at\"))\n",
        "    .groupBy(\"user_id\")\n",
        "    .agg(F.max(\"created_at\").alias(\"last_active_date\"))\n",
        ")\n",
        "\n",
        "# âœ… Build final user_snapshot (only once)\n",
        "user_snapshot = (\n",
        "    user_activity.join(posts_feedback, \"user_id\", \"left\")\n",
        "    .join(last_activity, \"user_id\", \"left\")\n",
        "    .withColumn(\"account_age_days\", F.datediff(F.current_date(), F.col(\"registration_date\")))\n",
        "    .fillna(0)\n",
        "    .withColumn(\"snapshot_date\", F.lit(date.today()))\n",
        ")\n",
        "\n",
        "# user_segment classification\n",
        "user_snapshot = user_snapshot.withColumn(\n",
        "    \"user_segment\",\n",
        "    F.when(F.col(\"account_age_days\") <= 30, \"New User\")\n",
        "     .when(F.col(\"last_active_date\") < F.date_sub(F.current_date(), 90), \"Churned\")\n",
        "     .when(F.col(\"last_active_date\") < F.date_sub(F.current_date(), 30), \"At Risk\")\n",
        "     .otherwise(\"Casual User\")\n",
        ")\n",
        "\n",
        "user_snapshot.repartition(\"snapshot_date\").write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"snapshot_date\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(\"gold_user_snapshot.parquet\")\n",
        "\n",
        "print(\"âœ… gold_user_snapshot created\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-OPpsXkQeEV",
        "outputId": "0c1b526a-3a46-4a9a-bb03-6db7efe3d37b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… gold_post_performance created\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 3ï¸âƒ£ GOLD: POST PERFORMANCE\n",
        "# ============================================================\n",
        "first_like = likes.groupBy(\"post_id\").agg(F.min(\"created_at\").alias(\"first_like_time\"))\n",
        "first_comment = comments.groupBy(\"post_id\").agg(F.min(\"created_at\").alias(\"first_comment_time\"))\n",
        "\n",
        "post_perf = (\n",
        "    posts\n",
        "    .join(broadcast(users.select(\"user_id\", \"username\")), \"user_id\", \"left\")\n",
        "    .join(broadcast(first_like), \"post_id\", \"left\")\n",
        "    .join(broadcast(first_comment), \"post_id\", \"left\")\n",
        "    .join(broadcast(likes.groupBy(\"post_id\").agg(F.count(\"*\").alias(\"total_likes\"))), \"post_id\", \"left\")\n",
        "    .join(broadcast(comments.groupBy(\"post_id\").agg(F.count(\"*\").alias(\"total_comments\"))), \"post_id\", \"left\")\n",
        "    .withColumn(\"time_to_first_like_minutes\",\n",
        "                F.round((F.unix_timestamp(\"first_like_time\") - F.unix_timestamp(\"created_at\")) / 60))\n",
        "    .withColumn(\"time_to_first_comment_minutes\",\n",
        "                F.round((F.unix_timestamp(\"first_comment_time\") - F.unix_timestamp(\"created_at\")) / 60))\n",
        "    .select(\n",
        "        \"post_id\",\n",
        "        F.to_date(\"created_at\").alias(\"created_date\"),\n",
        "        F.col(\"content\").alias(\"post_content\"),\n",
        "        F.col(\"sentiment\").alias(\"post_sentiment\"),\n",
        "        \"user_id\", \"username\",\n",
        "        \"total_likes\", \"total_comments\",\n",
        "        \"time_to_first_like_minutes\", \"time_to_first_comment_minutes\"\n",
        "    )\n",
        ")\n",
        "\n",
        "post_perf.repartition(\"created_date\").write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"created_date\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(\"gold_post_performance.parquet\")\n",
        "\n",
        "print(\"âœ… gold_post_performance created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCtOWK7pQfFw",
        "outputId": "7b445966-1880-40d1-b214-7a0060469485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Silver â†’ Gold transformation complete.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 4ï¸âƒ£ GOLD: CONTENT TRENDS (Optional)\n",
        "# ============================================================\n",
        "if \"keywords\" in posts.columns:\n",
        "    exploded_posts = posts.withColumn(\"topic_or_keyword\", F.explode(F.col(\"keywords\")))\n",
        "    exploded_comments = comments.withColumn(\"topic_or_keyword\", F.explode(F.col(\"keywords\")))\n",
        "\n",
        "    content_trends = (\n",
        "        exploded_posts.groupBy(F.to_date(\"created_at\").alias(\"report_date\"), \"topic_or_keyword\")\n",
        "        .agg(F.count(\"*\").alias(\"mention_count_in_posts\"))\n",
        "        .join(\n",
        "            exploded_comments.groupBy(F.to_date(\"created_at\").alias(\"report_date\"), \"topic_or_keyword\")\n",
        "            .agg(F.count(\"*\").alias(\"mention_count_in_comments\")),\n",
        "            [\"report_date\", \"topic_or_keyword\"],\n",
        "            \"outer\"\n",
        "        )\n",
        "        .fillna(0)\n",
        "        .withColumn(\"total_mentions\", F.col(\"mention_count_in_posts\") + F.col(\"mention_count_in_comments\"))\n",
        "        .withColumn(\"avg_sentiment_when_mentioned\", F.lit(None).cast(\"float\"))\n",
        "        .withColumn(\"trending_rank\",\n",
        "                    F.row_number().over(Window.partitionBy(\"report_date\").orderBy(F.desc(\"total_mentions\"))))\n",
        "    )\n",
        "    content_trends.repartition(\"report_date\").write.mode(\"overwrite\") \\\n",
        "        .partitionBy(\"report_date\") \\\n",
        "        .option(\"compression\", \"snappy\") \\\n",
        "        .parquet(\"gold_daily_content_trends.parquet\")\n",
        "\n",
        "print(\"âœ… Silver â†’ Gold transformation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMLUJ2ehnc5",
        "outputId": "8f60dd2c-f41a-43b8-dc2d-df7271226d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GOLD DAILY PLATFORM SUMMARY ===\n",
            "+-----------+--------------+--------------------+--------------------+--------------+-----------------+-----------------------+-----------------------+-----------+-------------+---------------------+-------------------+------------------+-----------+\n",
            "|total_posts|active_posters|positive_posts_count|negative_posts_count|total_comments|active_commenters|positive_comments_count|negative_comments_count|total_likes|active_likers|avg_comments_per_post|avg_likes_per_post |total_active_users|report_date|\n",
            "+-----------+--------------+--------------------+--------------------+--------------+-----------------+-----------------------+-----------------------+-----------+-------------+---------------------+-------------------+------------------+-----------+\n",
            "|2130       |2114          |682                 |483                 |933           |930              |291                    |232                    |862        |859          |0.43802816901408453  |0.40469483568075115|3870              |2023-07-22 |\n",
            "|2172       |2138          |712                 |525                 |925           |921              |316                    |205                    |901        |897          |0.4258747697974217   |0.4148250460405157 |3901              |2024-02-17 |\n",
            "|2138       |2122          |715                 |513                 |899           |896              |280                    |221                    |853        |851          |0.4204864359214219   |0.3989710009354537 |3828              |2023-12-17 |\n",
            "|2111       |2092          |704                 |481                 |935           |933              |274                    |218                    |853        |851          |0.44291804831833254  |0.4040738986262435 |3825              |2024-06-13 |\n",
            "|2159       |2137          |701                 |468                 |940           |931              |322                    |205                    |906        |900          |0.4353867531264474   |0.41963872163038446|3910              |2024-09-29 |\n",
            "+-----------+--------------+--------------------+--------------------+--------------+-----------------+-----------------------+-----------------------+-----------+-------------+---------------------+-------------------+------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "=== GOLD USER SNAPSHOT ===\n",
            "+-------+--------------------+--------------------+----------------+------------+\n",
            "|user_id|username            |total_posts_per_user|account_age_days|user_segment|\n",
            "+-------+--------------------+--------------------+----------------+------------+\n",
            "|2      |tran.hai2           |18                  |230             |Churned     |\n",
            "|3      |pham.long3          |15                  |219             |Churned     |\n",
            "|10     |truong.thanh.giang10|15                  |97              |Churned     |\n",
            "|12     |vu.lan12            |12                  |156             |Churned     |\n",
            "|16     |tran.trong.hung16   |15                  |55              |Churned     |\n",
            "+-------+--------------------+--------------------+----------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "=== GOLD POST PERFORMANCE ===\n",
            "+-------+-------+--------------+-----------+--------------+\n",
            "|post_id|user_id|post_sentiment|total_likes|total_comments|\n",
            "+-------+-------+--------------+-----------+--------------+\n",
            "|5019498|79994  |negative      |NULL       |NULL          |\n",
            "|5018492|88294  |neutral       |NULL       |NULL          |\n",
            "|5020717|4354   |positive      |NULL       |NULL          |\n",
            "|5018308|77404  |negative      |NULL       |NULL          |\n",
            "|5019581|45992  |positive      |NULL       |NULL          |\n",
            "+-------+-------+--------------+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "=== GOLD DAILY CONTENT TRENDS ===\n",
            "+-----------+----------------+--------------+-------------+\n",
            "|report_date|topic_or_keyword|total_mentions|trending_rank|\n",
            "+-----------+----------------+--------------+-------------+\n",
            "|2024-11-15 |data            |788           |1            |\n",
            "|2024-11-15 |mobile          |788           |2            |\n",
            "|2024-11-15 |technology      |788           |3            |\n",
            "|2024-11-15 |football        |750           |4            |\n",
            "|2024-11-15 |health          |750           |5            |\n",
            "|2024-11-15 |sports          |750           |6            |\n",
            "|2024-11-15 |ai              |737           |7            |\n",
            "|2024-11-15 |innovation      |737           |8            |\n",
            "|2024-11-15 |machine learning|737           |9            |\n",
            "|2024-11-15 |culture         |723           |10           |\n",
            "+-----------+----------------+--------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load and verify outputs\n",
        "gold_daily = spark.read.parquet(\"gold_daily_platform_summary.parquet\")\n",
        "gold_user = spark.read.parquet(\"gold_user_snapshot.parquet\")\n",
        "gold_post = spark.read.parquet(\"gold_post_performance.parquet\")\n",
        "gold_trend = spark.read.parquet(\"gold_daily_content_trends.parquet\")\n",
        "\n",
        "print(\"=== GOLD DAILY PLATFORM SUMMARY ===\")\n",
        "gold_daily.show(5, truncate=False)\n",
        "\n",
        "print(\"\\n=== GOLD USER SNAPSHOT ===\")\n",
        "gold_user.select(\"user_id\", \"username\", \"total_posts_per_user\", \"account_age_days\", \"user_segment\").show(5, truncate=False)\n",
        "\n",
        "print(\"\\n=== GOLD POST PERFORMANCE ===\")\n",
        "gold_post.select(\"post_id\", \"user_id\", \"post_sentiment\", \"total_likes\", \"total_comments\").show(5, truncate=False)\n",
        "\n",
        "print(\"\\n=== GOLD DAILY CONTENT TRENDS ===\")\n",
        "gold_trend.select(\"report_date\", \"topic_or_keyword\", \"total_mentions\", \"trending_rank\").show(10, truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
